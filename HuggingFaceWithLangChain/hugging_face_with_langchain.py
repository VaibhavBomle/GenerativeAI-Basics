# -*- coding: utf-8 -*-
"""hugging_face_with_langchain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N8bl9LdQuf520b8WUQ9ManWP9UJ_qFgK

**Install all required Library**
"""

!pip install huggingface_hub
!pip install transformers
!pip install accelerate
!pip install bitsandbytes
!pip install langchain

"""**Import required library**"""

from langchain import PromptTemplate, HuggingFaceHub, LLMChain

"""**Setting the Environment**"""

from google.colab import userdata
HUGGING_FACE_API = userdata.get('HUGGING_FACE_API')

"""**Text2ext Generation Models | Seq2Seq Models | Encoder-Decoder Models**"""

prompt = PromptTemplate(
    input_variables = ["product"],
    template = "What is good name for a company that makes {product}"
)

"""**google/flan-t5-base**"""

chain = LLMChain(llm = HuggingFaceHub(repo_id= 'google/flan-t5-base',huggingfacehub_api_token=HUGGING_FACE_API,model_kwargs={'temperature':0}),prompt=prompt)

chain.run("Colorful clothes")

"""**facebook/mbart-large-50**"""

chain2 = LLMChain(llm = HuggingFaceHub(repo_id= 'facebook/mbart-large-50',huggingfacehub_api_token=HUGGING_FACE_API,model_kwargs={'temperature':0}),prompt=prompt)

chain2.run("Mobile")

"""**Text Generation Models | Decoder Only Models**"""

from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_id = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=128)
hf = HuggingFacePipeline(pipeline=pipe)
# prompt = PromptTemplate(
#            input_variables = ["name"],
#            template = "can you tell me about footballer {name}"
#            )

from langchain.prompts import PromptTemplate

template = """Question: {question}"""
prompt = PromptTemplate.from_template(template)
#chain = LLMChain(llm=hf, prompt=prompt)
chain = prompt | hf

question = "What is the capital city of India?"

print(chain.invoke({"question": question}))

# Issue in below approach
# from langchain.llms import HuggingFacePipeline
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline,AutoModelForSeq2SeqLM
# model_id = "google/flan-t5-large"

# tokenizer = AutoTokenizer.from_pretrained(model_id)

# model = AutoModelForSeq2SeqLM.from_pretrained(model_id,device_map = 'auto')

# pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_length=128)

# local_llm = HuggingFacePipeline(pipeline=pipeline)

# prompt = PromptTemplate(
#            input_variables = ["name"],
#            template = "can you tell me about footballer {name}"
#            )

# chain = LLMChain(llm=local_llm, prompt=prompt)
# chain.invoke("Sunil ")